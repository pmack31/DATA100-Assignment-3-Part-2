---
title: "Correlation between happiness and climate change"
subtitle: "Exploring Disparate Data: Part 3 - Final Report"
author: "Old Souls"
date: "Due November 26th, 2025"
output: pdf_document
---

List your group members, including their student numbers, here:

-   Peter MacKenzie (169139553)
-   Frank Castiglione (169079592)
-   Josh Di Maulo (169112454)

# Data Description

## \<\<Cyclones\>\>

```{r load_data1}

# Set the base URL for NOAA cyclone data
cyclone_data_address <- "https://www.nhc.noaa.gov/data/hurdat/"
# Define filenames for Atlantic and Northeast Pacific cyclone datasets
at_cyclone_filename <- "hurdat2-1851-2022-050423.txt"
np_cyclone_filename <- "hurdat2-nepac-1949-2022-050423.txt"

# Define column names for the detailed observation data that will be parsed
new_columns <- c("status", "latitude", "longitude", "max_wind",
    "min_pressure", "NE_extend_34", "SE_extend_34", "SW_extend_34",
    "NW_extend_34", "NE_extend_50", "SE_extend_50", "SW_extend_50",
    "NW_extend_50", "NE_extend_64", "SE_extend_64", "SW_extend_64",
    "NW_extend_64", "r_max_wind"
)

#Process Atlantic cyclone data
at_cyclone <- str_c(cyclone_data_address, at_cyclone_filename, sep = "") |>
  # Read the CSV file with temporary column names (1-4)  
  read_csv(
        col_names = c(as.character(1:4)),
        progress = FALSE,
        show_col_types = FALSE
    ) |>
  # Split the 4th column into multiple columns
    separate_wider_delim(
        cols = `4`,
        # Set the delim and the names
        delim = ",",
        names = new_columns
    ) |>
  #Data cleaning
    mutate(
      # Remove whitespace from all columns
        across(everything(), str_trim),
        # Convert missing values to NA
        across(everything(), ~na_if(., "-999")),
        across(everything(), ~na_if(., "-99")),
        # Create storm data columns
        BasinNumberYear = ifelse(is.na(status), `1`, NA),
        Name = ifelse(is.na(status), `2`, NA),
        Entries = ifelse(is.na(status), `3`, NA)
    ) |>
  # Reorganize columns to put data first
    relocate(BasinNumberYear, Name, Entries) |>
  # Fill down data values to observation rows for each storm
    fill(BasinNumberYear, Name, Entries) |>
  # Remove header rows, keeping only observation rows
    filter(!is.na(status))  |>
  # Remove the Entries column as it is no longer needed
    select(-Entries) |>
  # Split BasinNumberYear into its components
    separate_wider_position(
        BasinNumberYear,
        widths = c(
        Basin = 2,   # Basin code
        Number = 2,  # Storm number for the year
        NameYear = 4 # Year of the storm
        )
    ) |>
  # Split date information from column 1
    separate_wider_position(
        `1`,
        widths = c(
        ObservYear = 4,  # Observation year
        Month = 2,       # Observation month
        Day = 2          # Observation day
        )
    ) |>
  # Split time information from column 2
    separate_wider_position(
        `2`,
        widths = c(
          Hour = 2,    # Observation hour
          Minute = 2   # Observation minute
        )
    ) |>
  # Rename column 3 to more better name
    rename(
        Identifier = `3`   # Record identifier
    ) |>
  # Convert character columns to integers
    mutate(
        across(
            c(NameYear, ObservYear, Month, Day, Hour,
                Minute, Number),
            as.integer
        )
    ) |>
  # Convert measurement columns to numeric
    mutate(across(max_wind:r_max_wind, as.numeric))

# Process Northeast Pacific cyclone data using the same steps as above
np_cyclone <- str_c(cyclone_data_address, np_cyclone_filename, sep = "") |>
    # ALL of the steps all over again
    read_csv(
        col_names = c(as.character(1:4)),
        progress = FALSE,
        show_col_types = FALSE
    ) |>
    separate_wider_delim(
        cols = `4`,
        # Set the delim and the names
        delim = ",",
        names = new_columns
    ) |>
    mutate(
        across(everything(), str_trim),
        # make "-999" NAs, make "-99" NAs
        # Create columns BasinNumberYear, Name, and Entries
        across(everything(), ~na_if(., "-999")),
        across(everything(), ~na_if(., "-99")),
        BasinNumberYear = ifelse(is.na(status), `1`, NA),
        Name = ifelse(is.na(status), `2`, NA),
        Entries = ifelse(is.na(status), `3`, NA)
    ) |>
    relocate(BasinNumberYear, Name, Entries) |>
    fill(BasinNumberYear, Name, Entries) |>
    filter(!is.na(status))  |>
    select(-Entries) |>
    separate_wider_position(
        BasinNumberYear,
        # Specify the widths
        widths = c(
        Basin = 2,
        Number = 2,
        NameYear = 4
        )
    ) |>
    separate_wider_position(
        `1`,
        # Specify the widths
        widths = c(
        ObservYear = 4,
        Month = 2,
        Day = 2
        )
    ) |>
    separate_wider_position(
        `2`,
        # Specify the widths
        widths = c(
          Hour = 2,
          Minute = 2
        )
    ) |>
    rename(
        Identifier = `3`
    ) |>
    mutate(
        across(
            c(NameYear, ObservYear, Month, Day, Hour,
                Minute, Number),
            as.integer
        )
    ) |>
    mutate(across(max_wind:r_max_wind, as.numeric))

# Combine Atlantic and Pacific data into a single dataset
cyclones_data_update_0 <- bind_rows(at_cyclone, np_cyclone)

# Helper function to convert latitude/longitude from string to numeric
# Handles directions by applying negative sign for W and S
convert_latlon <- function(latlon) {
    parse_number(latlon) * if_else(str_detect(latlon, "[WS]"), -1, 1
  )
}

# Apply coordinate conversion
cyclones_data_update_1 <- cyclones_data_update_0 |>
    mutate(
        lat = convert_latlon(latitude),   # Convert latitude to numeric
        lon = convert_latlon(longitude)   # Convert longitude to numeric
    )

# Create proper datetime column from date/time components
cyclones_data_update_2 <- cyclones_data_update_1 |>
    mutate(
        date = make_datetime(ObservYear, Month, Day, Hour, Minute)
    )

# Define category levels for hurricane classification
cat_levels <- c("TD", "TS", "1", "2", "3", "4", "5")

# Assign hurricane categories based on maximum wind speed
cyclones_data <- cyclones_data_update_2 |>
    mutate(
        category = ordered(
            case_when(
                (max_wind <= 33) ~ "TD",  # Tropical Depression
                (max_wind <= 63) ~ "TS",  # Tropical Storm
                (max_wind <= 82) ~ "1",   # Category 1 Hurricane
                (max_wind <= 95) ~ "2",   # Category 2 Hurricane
                (max_wind <= 112) ~ "3",  # Category 3 Hurricane
                (max_wind <= 136) ~ "4",  # Category 4 Hurricane
                (max_wind >= 137) ~ "5"   # Category 5 Hurricane
                ),
            levels = cat_levels # set levels as ordered factor
        )
    )

# save as parquet
write_parquet(cyclones_data, sink = "cyclones_data.parquet")

# Note that the code in this document will not be shown
# when you click "knit", so the placement of this code
# chunk is purely for your benefit: You can see what happened
# with your data, which makes it easier to describe below!
```

The data come from \<<place>\> and describe \<<more specific description of the data>\>.

In order to clean the data, we \<\<steps to clean the data, concise but precise enough that a reader could follow your steps without seeing your code\>\>.

## \<\<Climate Change\>\>

```{r load_data2}
# Set URL for climate opinion survey dataset
climate_opinion_address <- "https://data.humdata.org/dataset/dc9f2ca4-8b62-4747-89b1-db426ce617a0/resource/6041db5f-8190-47ff-a10b-9841325de841/download/climate_change_opinion_survey_2022_aggregated.xlsx"

# Get the names of all sheets in the Excel workbook
climate_sheet_names <- climate_opinion_address |>
    loadWorkbook() |>
    names()

# Define the sheet name we want to work with
aware_sheet_name <- "climate_awareness"

# Process the climate awareness data
climate_awareness <- climate_opinion_address |>
  # Read the sheet from the Excel file
    read.xlsx(
        sheet = aware_sheet_name
    ) |>
    # Transform the data from wide to long
    # Keep the climate_awareness column as is, pivot all other columns
    pivot_longer(
        cols = !contains(aware_sheet_name),  # Select all columns except "climate_awareness"
        names_to = "country",  # Column names become values in new "country" column
        values_to = "score"    # Cell values become values in new "score" column
    ) |>
  # Clean and standardize the response categories
    mutate(
        climate_awareness = case_when(
          # Convert responses to concise, standardized codes
            climate_awareness == "I have never heard of it" ~ "aware_no",
            climate_awareness == "I know a little about it" ~ "aware_alittle",
            climate_awareness == "I know a moderate amount about it" ~
                "aware_moderate",
            climate_awareness == "I know a lot about it" ~ "aware_alot",
            climate_awareness == "Refused" ~ "aware_refuse",
            climate_awareness == "(Unweighted Base)" ~ "aware_base"
        )
    ) |>
  # Rename the main categorical column to "answer"
    rename(answer = climate_awareness) |>
  # Transform back to wide format with response categories as separate columns
    pivot_wider(
        names_from = answer,
        values_from = score
    )

# Save the processed data in parquet format
write_parquet(climate_awareness, "climate_awareness.parquet")

# Reminder: do NOT print your data to the screen unless it's
# completely necessary
```

The data comes from \<<the Climate Change Opinion Survey in 2022, including more than 100 countries' citizens, who gave valuable information about awareness and changes in their attitude towards climate change. The results prove that human activities are a major contributor to climate change. In Latin America, Europe, and on small island nations, the majority of citizens have a high level of concern about climate change. It is believed that in the next 20 years, climate change will cause harm to both current citizens and future generations. To solve this issue, the majority of concern lies with governments and businesses, but many individuals say they would also take action if necessary. It is seen to have support to transition toward renewable energy and transition away from fossil fuels to reduce harm caused towards climate change, and the data reflects that climate change is human-caused and requires immediate, collective action.>\>.

In order to clean the data, we \<\<This R script corresponds to the Climate Change Opinion Survey 2022 Excel Dataset to read in, clean and reshape the dataset. Therefore, it first accesses the Humanitarian Data Exchange public file that is the public dataset and then provides the file path to access the workbook directly. The loadWorkbook() function is then initiated to determine the sheet names within the recognized excel workbook. Then the sheet name climate_awareness is extracted from this list to proceed with the remainder of file access and reshaping concerning the dataset. The read.xlsx() function reads the relevant sheet of the excel workbook into R and the pivot_longer() function reshapes the file from wide to long, taking country column names and creating a new variable name "country", while keeping "climate_awareness" as is for the categorical variable. This crossjoins each uniquely-response category with different country's scores. For example, one response category might be "I know a little about it" and this will appear on separate rows for separate countries with different scores. Therefore, in order to make response categories more manageable, the mutate() and case_when() functions are used in conjunction to re-code long survey responses into shorter response codes. For example, "I know a little about it" is now re-coded to aware_alittle. Similar adjustments were made for other response categories. After this re-coding, the main column is renamed from “climate_awareness” to “answer” to bring clarity and consistency. The dataset is then changed back to wide format using pivot_wider(), which will create separate columns for each category with their respective scores. Finally, the cleaned and transformed data is exported and saved in Parquet format using write_parquet(), which allows storage and compatibility for future analysis.\>\>

## \<\<Happiness\>\>

```{r load_data3}
# read excel fie containing the happiness data
happiness <- read.xlsx("DataForTable2.1.xlsx") |>
  # Clean column names
    janitor::clean_names() |>
  # Rename "country_name" to "country" for consistency
    rename(country = country_name) |>
  # Remove rows with missing happiness scores
    filter(!is.na(ladder_score)) |>
  # Group data by country to process each country separately
    group_by(country) |>
  # Sort years in descending order within each country
    arrange(desc(year)) |>
  # Remove grouping for future operations
    ungroup()

```

The data comes from \<<the World Happiness Report 2023, explaining that global wellbeing is assessed in an additional 150 countries through surveys like Gallup World Poll. They additionally clarify how happiness is assessed through social support, freedom of choices in life, perceived corruption, life expectancy and GDP per capita. They assert that on a national level, social support and institutional credibility and social networks/community/group endeavors increase happiness averages. Furthermore, they assert income with social equity best sustains happiness over time as economic growth shows increased income is insufficient when simultaneously assessed for inequality and fear. Furthermore, the report assesses regional averages with regional means which show the Nordic countries at the top time and again as they are highly assessed in trustworthy and governmental abilities. Furthermore, a consistent theme is resilience in assessments as researchers assess countries that either maintained happiness or reduced their numbers amid international shocks (shock being defined as pandemic, financial crisis or geopolitical conflict). Finally, in 2023, a consistent shift in data has occurred post-pandemic assessing psychological recovery or simple aging (decline). Finally, the appendices show the value of country-to-country comparisons as well as over time for researchers with statistical validity and additional statistical assessments and credibility.>\> and detail \<<more specific description of the data>\>.

In order to clean the data, we \<\<The following R code loads and cleans the World Happiness Report dataset from the Excel document DataForTable2.1.xlsx. To begin, the read.xlsx() function reads the unedited excel document into R as is. Then, for purposes of easier transformations, janitor::clean_names() functions to make all column titles lower case, replace spaces with underscores, and eliminate special characters. The third aspect of this function renames the country_name title to country in order to maintain title uniformity amongst the various World Happiness Report excel documents and provides clearer referencing for analyses further down the road. Lastly, the code omits missing values from the ladder_score column by using the filter() function to exclude these scores. Once such actions are taken, the dataset is grouped by country using group_by(country) so that subsequent analyses can take place one country at a time. Once grouped, the dataset is sorted with arrange(desc(year)), which allows each country’s observations to be ordered from the most recent year to the oldest year. Finally, the ungroup() function is used to remove the current grouping, ensuring that future moves apply to the dataset as a whole rather than within each country.\>\>

## Combining the Data

Explain how any combinations of data were performed. Explain what kind of join was needed, whether columns had to be modified (for example, matching "country" names.)

# Exploratory Data Analysis

To achieve our goals, we explored the data by...

We explored many aspects of the data, but will demonstrate three. These are \<\<insight 1\>\>, \<\<insight 2\>\>, and \<<insight3>\>

The first aspect that we found interesting is shown in \@ref(fig:insight1). The insight should be specific to the data shown, not a general statement beyond the data (leave that for the conclusion).

```{r insight1, fig.cap="This is a figure caption that you will need to change in order to get good marks in the visualization rubric items."}
# This is an example of how you can control figures and captions in
# an R chunk. Note that you can reference figures using:
# \@ref(fig:insight1), where "insight1" is the label of this code
# chunk (the first bit of text after the "r" in "```{r label, options...}")
```

This insight is supported by the summary statistics in table \@ref(tab:summary_stats)

```{r summary_stats}
# Calculate the relevant summary statistics here.
# Note that the "kable" function in the "knitr" package
# is convenient for making nice tables. Other packages can
# do much fancier things with tables, but keep in mind that
# the insights should be the star, not the formatting.
```

The next insight that we found is shown in \@ref(fig:insight2).

```{r insight2, fig.height=4, fig.width=6, fig.cap="This is a figure caption that you will need to change in order to get good marks in the visualization rubric items."}
# This figure will have a height of 4 and a width of 6.
# Feel free to change this, and to apply different sizes
# to the other figures you create.
```

Finally, \@ref(fig:insight3) shows ...

```{r insight3, fig.height=4, fig.width=6, fig.cap="This is a figure caption that you will need to change in order to get good marks in the visualization rubric items."}
```

# Conclusion and Future Work

Overall, we found \<<general ideas>\>.

A second paragraph about our findings.

The next steps in this analysis are...

The limitations of this analysis are as follows. (Do not simply list potential issues with sampling, but relate them to your analysis and how they affect your conclusions. An honest and complete acknowledgement of the limitations makes the analysis more trustworthy.)

# References

I am not strict about MLA or APA style or anything like that. For this report, I would much rather have your citations be easy to match to your insights.

The easiest way is to use Rmd's [footnote](https://bookdown.org/yihui/rmarkdown/markdown-syntax.html#inline-formatting) syntax. This will put a number beside the word where the footnote appears, and the full text of the footnote at the bottom of the page (pdf) or end of the document (html). The syntax is:[^1], where I suggest that you put in something like this[^2] to make references for this assignment.

[^1]: See the source view to see this footnote

[^2]: The relevance to the insight is ... . From \<<name of source and name of article>\>, published on \<<date>\>, url: \<<link to page>\>

Alternatively, you could make a list of citations with their main arguments and why they're relevent to your insights, methods, etc.

The link above also references "bibtex" files. These are also extremely convenient, but have a steep learning curve and they make it difficult to tie them to an insight. If you use bibtext, then make sure that you provide a sentence to describe the source and it's relevance when you cite it - don't just add citations to the end of a sentence (this is common practice in academia, but I want to know that your citations are directly relevant for this assignmnet).
