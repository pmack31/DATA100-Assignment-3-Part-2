---
title: "Correlation between happiness and climate change"
subtitle: "Exploring Disparate Data: Part 3 - Final Report"
author: "Old Souls"
date: "Due November 26th, 2025"
output: pdf_document
---

List your group members, including their student numbers, here:

-   Peter MacKenzie (169139553)
-   Frank Castiglione (169079592)
-   Josh Di Maulo (169112454)

# Data Description

## \<\<Cyclones\>\>

```{r load_data1}

# Set the base URL for NOAA cyclone data
cyclone_data_address <- "https://www.nhc.noaa.gov/data/hurdat/"
# Define filenames for Atlantic and Northeast Pacific cyclone datasets
at_cyclone_filename <- "hurdat2-1851-2022-050423.txt"
np_cyclone_filename <- "hurdat2-nepac-1949-2022-050423.txt"

# Define column names for the detailed observation data that will be parsed
new_columns <- c("status", "latitude", "longitude", "max_wind",
    "min_pressure", "NE_extend_34", "SE_extend_34", "SW_extend_34",
    "NW_extend_34", "NE_extend_50", "SE_extend_50", "SW_extend_50",
    "NW_extend_50", "NE_extend_64", "SE_extend_64", "SW_extend_64",
    "NW_extend_64", "r_max_wind"
)

#Process Atlantic cyclone data
at_cyclone <- str_c(cyclone_data_address, at_cyclone_filename, sep = "") |>
  # Read the CSV file with temporary column names (1-4)  
  read_csv(
        col_names = c(as.character(1:4)),
        progress = FALSE,
        show_col_types = FALSE
    ) |>
  # Split the 4th column into multiple columns
    separate_wider_delim(
        cols = `4`,
        # Set the delim and the names
        delim = ",",
        names = new_columns
    ) |>
  #Data cleaning
    mutate(
      # Remove whitespace from all columns
        across(everything(), str_trim),
        # Convert missing values to NA
        across(everything(), ~na_if(., "-999")),
        across(everything(), ~na_if(., "-99")),
        # Create storm data columns
        BasinNumberYear = ifelse(is.na(status), `1`, NA),
        Name = ifelse(is.na(status), `2`, NA),
        Entries = ifelse(is.na(status), `3`, NA)
    ) |>
  # Reorganize columns to put data first
    relocate(BasinNumberYear, Name, Entries) |>
  # Fill down data values to observation rows for each storm
    fill(BasinNumberYear, Name, Entries) |>
  # Remove header rows, keeping only observation rows
    filter(!is.na(status))  |>
  # Remove the Entries column as it is no longer needed
    select(-Entries) |>
  # Split BasinNumberYear into its components
    separate_wider_position(
        BasinNumberYear,
        widths = c(
        Basin = 2,   # Basin code
        Number = 2,  # Storm number for the year
        NameYear = 4 # Year of the storm
        )
    ) |>
  # Split date information from column 1
    separate_wider_position(
        `1`,
        widths = c(
        ObservYear = 4,  # Observation year
        Month = 2,       # Observation month
        Day = 2          # Observation day
        )
    ) |>
  # Split time information from column 2
    separate_wider_position(
        `2`,
        widths = c(
          Hour = 2,    # Observation hour
          Minute = 2   # Observation minute
        )
    ) |>
  # Rename column 3 to more better name
    rename(
        Identifier = `3`   # Record identifier
    ) |>
  # Convert character columns to integers
    mutate(
        across(
            c(NameYear, ObservYear, Month, Day, Hour,
                Minute, Number),
            as.integer
        )
    ) |>
  # Convert measurement columns to numeric
    mutate(across(max_wind:r_max_wind, as.numeric))

# Process Northeast Pacific cyclone data using the same steps as above
np_cyclone <- str_c(cyclone_data_address, np_cyclone_filename, sep = "") |>
    # ALL of the steps all over again
    read_csv(
        col_names = c(as.character(1:4)),
        progress = FALSE,
        show_col_types = FALSE
    ) |>
    separate_wider_delim(
        cols = `4`,
        # Set the delim and the names
        delim = ",",
        names = new_columns
    ) |>
    mutate(
        across(everything(), str_trim),
        # make "-999" NAs, make "-99" NAs
        # Create columns BasinNumberYear, Name, and Entries
        across(everything(), ~na_if(., "-999")),
        across(everything(), ~na_if(., "-99")),
        BasinNumberYear = ifelse(is.na(status), `1`, NA),
        Name = ifelse(is.na(status), `2`, NA),
        Entries = ifelse(is.na(status), `3`, NA)
    ) |>
    relocate(BasinNumberYear, Name, Entries) |>
    fill(BasinNumberYear, Name, Entries) |>
    filter(!is.na(status))  |>
    select(-Entries) |>
    separate_wider_position(
        BasinNumberYear,
        # Specify the widths
        widths = c(
        Basin = 2,
        Number = 2,
        NameYear = 4
        )
    ) |>
    separate_wider_position(
        `1`,
        # Specify the widths
        widths = c(
        ObservYear = 4,
        Month = 2,
        Day = 2
        )
    ) |>
    separate_wider_position(
        `2`,
        # Specify the widths
        widths = c(
          Hour = 2,
          Minute = 2
        )
    ) |>
    rename(
        Identifier = `3`
    ) |>
    mutate(
        across(
            c(NameYear, ObservYear, Month, Day, Hour,
                Minute, Number),
            as.integer
        )
    ) |>
    mutate(across(max_wind:r_max_wind, as.numeric))

# Combine Atlantic and Pacific data into a single dataset
cyclones_data_update_0 <- bind_rows(at_cyclone, np_cyclone)

# Helper function to convert latitude/longitude from string to numeric
# Handles directions by applying negative sign for W and S
convert_latlon <- function(latlon) {
    parse_number(latlon) * if_else(str_detect(latlon, "[WS]"), -1, 1
  )
}

# Apply coordinate conversion
cyclones_data_update_1 <- cyclones_data_update_0 |>
    mutate(
        lat = convert_latlon(latitude),   # Convert latitude to numeric
        lon = convert_latlon(longitude)   # Convert longitude to numeric
    )

# Create proper datetime column from date/time components
cyclones_data_update_2 <- cyclones_data_update_1 |>
    mutate(
        date = make_datetime(ObservYear, Month, Day, Hour, Minute)
    )

# Define category levels for hurricane classification
cat_levels <- c("TD", "TS", "1", "2", "3", "4", "5")

# Assign hurricane categories based on maximum wind speed
cyclones_data <- cyclones_data_update_2 |>
    mutate(
        category = ordered(
            case_when(
                (max_wind <= 33) ~ "TD",  # Tropical Depression
                (max_wind <= 63) ~ "TS",  # Tropical Storm
                (max_wind <= 82) ~ "1",   # Category 1 Hurricane
                (max_wind <= 95) ~ "2",   # Category 2 Hurricane
                (max_wind <= 112) ~ "3",  # Category 3 Hurricane
                (max_wind <= 136) ~ "4",  # Category 4 Hurricane
                (max_wind >= 137) ~ "5"   # Category 5 Hurricane
                ),
            levels = cat_levels # set levels as ordered factor
        )
    )

# save as parquet
write_parquet(cyclones_data, sink = "cyclones_data.parquet")

# Note that the code in this document will not be shown
# when you click "knit", so the placement of this code
# chunk is purely for your benefit: You can see what happened
# with your data, which makes it easier to describe below!
```

The data come from \<<place>\> and describe \<<more specific description of the data>\>.

In order to clean the data, we \<\<steps to clean the data, concise but precise enough that a reader could follow your steps without seeing your code\>\>.

## \<\<Climate Change\>\>

```{r load_data2}
# Set URL for climate opinion survey dataset
climate_opinion_address <- "https://data.humdata.org/dataset/dc9f2ca4-8b62-4747-89b1-db426ce617a0/resource/6041db5f-8190-47ff-a10b-9841325de841/download/climate_change_opinion_survey_2022_aggregated.xlsx"

# Get the names of all sheets in the Excel workbook
climate_sheet_names <- climate_opinion_address |>
    loadWorkbook() |>
    names()

# Define the sheet name we want to work with
aware_sheet_name <- "climate_awareness"

# Process the climate awareness data
climate_awareness <- climate_opinion_address |>
  # Read the sheet from the Excel file
    read.xlsx(
        sheet = aware_sheet_name
    ) |>
    # Transform the data from wide to long
    # Keep the climate_awareness column as is, pivot all other columns
    pivot_longer(
        cols = !contains(aware_sheet_name),  # Select all columns except "climate_awareness"
        names_to = "country",  # Column names become values in new "country" column
        values_to = "score"    # Cell values become values in new "score" column
    ) |>
  # Clean and standardize the response categories
    mutate(
        climate_awareness = case_when(
          # Convert responses to concise, standardized codes
            climate_awareness == "I have never heard of it" ~ "aware_no",
            climate_awareness == "I know a little about it" ~ "aware_alittle",
            climate_awareness == "I know a moderate amount about it" ~
                "aware_moderate",
            climate_awareness == "I know a lot about it" ~ "aware_alot",
            climate_awareness == "Refused" ~ "aware_refuse",
            climate_awareness == "(Unweighted Base)" ~ "aware_base"
        )
    ) |>
  # Rename the main categorical column to "answer"
    rename(answer = climate_awareness) |>
  # Transform back to wide format with response categories as separate columns
    pivot_wider(
        names_from = answer,
        values_from = score
    )

# Save the processed data in parquet format
write_parquet(climate_awareness, "climate_awareness.parquet")

# Reminder: do NOT print your data to the screen unless it's
# completely necessary
```

The data come from \<<place>\> and detail \<<more specific description of the data>\>.

In order to clean the data, we \<\<steps to clean the data, concise but precise enough that a reader could follow your steps without seeing your code\>\>

## \<\<Happiness\>\>

```{r load_data3}
# read excel fie containing the happiness data
happiness <- read.xlsx("DataForTable2.1.xlsx") |>
  # Clean column names
    janitor::clean_names() |>
  # Rename "country_name" to "country" for consistency
    rename(country = country_name) |>
  # Remove rows with missing happiness scores
    filter(!is.na(ladder_score)) |>
  # Group data by country to process each country separately
    group_by(country) |>
  # Sort years in descending order within each country
    arrange(desc(year)) |>
  # Remove grouping for future operations
    ungroup()

```

The data come from \<<place>\> and detail \<<more specific description of the data>\>.

In order to clean the data, we \<\<steps to clean the data, concise but precise enough that a reader could follow your steps without seeing your code\>\>

## Combining the Data

Explain how any combinations of data were performed. Explain what kind of join was needed, whether columns had to be modified (for example, matching "country" names.)

# Exploratory Data Analysis

To achieve our goals, we explored the data by...

We explored many aspects of the data, but will demonstrate three. These are \<\<insight 1\>\>, \<\<insight 2\>\>, and \<<insight3>\>

The first aspect that we found interesting is shown in \@ref(fig:insight1). The insight should be specific to the data shown, not a general statement beyond the data (leave that for the conclusion).

```{r insight1, fig.cap="This is a figure caption that you will need to change in order to get good marks in the visualization rubric items."}
# This is an example of how you can control figures and captions in
# an R chunk. Note that you can reference figures using:
# \@ref(fig:insight1), where "insight1" is the label of this code
# chunk (the first bit of text after the "r" in "```{r label, options...}")
```

This insight is supported by the summary statistics in table \@ref(tab:summary_stats)

```{r summary_stats}
# Calculate the relevant summary statistics here.
# Note that the "kable" function in the "knitr" package
# is convenient for making nice tables. Other packages can
# do much fancier things with tables, but keep in mind that
# the insights should be the star, not the formatting.
```

The next insight that we found is shown in \@ref(fig:insight2).

```{r insight2, fig.height=4, fig.width=6, fig.cap="This is a figure caption that you will need to change in order to get good marks in the visualization rubric items."}
# This figure will have a height of 4 and a width of 6.
# Feel free to change this, and to apply different sizes
# to the other figures you create.
```

Finally, \@ref(fig:insight3) shows ...

```{r insight3, fig.height=4, fig.width=6, fig.cap="This is a figure caption that you will need to change in order to get good marks in the visualization rubric items."}
```

# Conclusion and Future Work

Overall, we found \<<general ideas>\>.

A second paragraph about our findings.

The next steps in this analysis are...

The limitations of this analysis are as follows. (Do not simply list potential issues with sampling, but relate them to your analysis and how they affect your conclusions. An honest and complete acknowledgement of the limitations makes the analysis more trustworthy.)

# References

I am not strict about MLA or APA style or anything like that. For this report, I would much rather have your citations be easy to match to your insights.

The easiest way is to use Rmd's [footnote](https://bookdown.org/yihui/rmarkdown/markdown-syntax.html#inline-formatting) syntax. This will put a number beside the word where the footnote appears, and the full text of the footnote at the bottom of the page (pdf) or end of the document (html). The syntax is:[^1], where I suggest that you put in something like this[^2] to make references for this assignment.

[^1]: See the source view to see this footnote

[^2]: The relevance to the insight is ... . From \<<name of source and name of article>\>, published on \<<date>\>, url: \<<link to page>\>

Alternatively, you could make a list of citations with their main arguments and why they're relevent to your insights, methods, etc.

The link above also references "bibtex" files. These are also extremely convenient, but have a steep learning curve and they make it difficult to tie them to an insight. If you use bibtext, then make sure that you provide a sentence to describe the source and it's relevance when you cite it - don't just add citations to the end of a sentence (this is common practice in academia, but I want to know that your citations are directly relevant for this assignmnet).
